# AI小说生成器 - 配置说明

## 🚀 快速开始

### 1. 本地Ollama配置（推荐，默认）
程序默认配置为使用本地Ollama模型，无需API密钥。

**安装Ollama：**
```bash
# macOS (Intel)
curl -fsSL https://ollama.com/install.sh | sh

# macOS (Apple Silicon)  
curl -fsSL https://ollama.com/install.sh | sh

# 或访问 https://ollama.com/download 下载安装
```

**下载模型：**
```bash
# 下载推荐的大语言模型
ollama pull gpt-oss:120b     # 120B参数模型，性能强大（默认）
ollama pull llama3.1:8b      # 8B参数模型，性能好
ollama pull qwen2.5:7b       # 阿里通义千问，中文优秀
ollama pull deepseek-coder   # 编程专用

# 下载向量嵌入模型
ollama pull nomic-embed-text:137m-v1.5-fp16  # 高精度嵌入模型（默认）
```

**启动Ollama服务：**
```bash
# 终端启动
ollama serve

# 或后台运行
nohup ollama serve &
```

### 2. DeepSeek在线配置
如果需要在线服务，可配置DeepSeek：

1. 访问 [DeepSeek](https://platform.deepseek.com/) 注册账号
2. 获取API密钥
3. 在程序配置中填写API密钥

## ⚙️ 配置详解

### 大语言模型配置

#### 本地Ollama（默认）
```json
{
    "api_key": "ollama",
    "base_url": "http://localhost:11434/v1", 
    "model_name": "gpt-oss:120b",
    "temperature": 0.7,
    "max_tokens": 8192,
    "timeout": 600
}
```

**可替换模型：**
- `gpt-oss:120b` - 120B参数，性能强大（默认）
- `llama3.1:8b` - 综合性能好
- `qwen2.5:7b` - 中文优秀
- `deepseek-coder` - 编程专用
- `codellama` - 代码生成

#### DeepSeek在线
```json
{
    "api_key": "sk-你的密钥",
    "base_url": "https://api.deepseek.com/v1",
    "model_name": "deepseek-chat",
    "temperature": 0.7,
    "max_tokens": 8192,
    "timeout": 600
}
```

### 向量嵌入配置

#### 本地Ollama（默认）
```json
{
    "api_key": "ollama",
    "base_url": "http://localhost:11434/v1",
    "model_name": "nomic-embed-text:137m-v1.5-fp16",
    "retrieval_k": 4
}
```

#### DeepSeek在线
```json
{
    "api_key": "sk-你的密钥", 
    "base_url": "https://api.deepseek.com/v1",
    "model_name": "deepseek-chat",
    "retrieval_k": 4
}
```

## 🔧 参数说明

| 参数 | 说明 | 推荐值 |
|------|------|--------|
| `temperature` | 创意度，0-1之间 | 0.7 (平衡创意与逻辑) |
| `max_tokens` | 单次生成最大token数 | 8192 |
| `retrieval_k` | 向量检索返回数量 | 4 |
| `timeout` | 请求超时时间(秒) | 600 |

## 🚨 常见问题

### Q1: Ollama连接失败？
**解决方案：**
1. 确认Ollama服务已启动：`ollama serve`
2. 检查端口：`http://localhost:11434`
3. 确认模型已下载：`ollama list`

### Q2: 生成速度慢？
**优化建议：**
1. 使用本地模型速度更快
2. 选择较小的模型（如7B参数）
3. 适当降低`max_tokens`值

### Q3: 中文效果不好？
**建议：**
1. 使用`qwen2.5:7b`模型，中文优秀
2. 调整`temperature`为0.8增加创意
3. 在用户指导中明确中文要求

## 🎯 最佳实践

1. **首次使用**：建议先使用Ollama本地模型
2. **性能优先**：推荐gpt-oss:120b（默认）性能强大
3. **轻量使用**：可选择llama3.1:8b或qwen2.5:7b
4. **质量优先**：可使用DeepSeek在线服务
5. **中文写作**：推荐qwen2.5:7b模型
6. **长文本**：确保max_tokens足够大

## 🔄 切换配置

在程序界面中：
1. 进入"配置选择"标签页
2. 选择不同的配置名称
3. 点击"保存配置"按钮
4. 重启程序生效

---

💡 **提示**：配置文件保存在 `config.json`，建议备份重要配置。